# SCRIPT TO CALCULATE AREAS AND ACCURACIES PER PERIOD
# (E.G. ANNUALLY, BIANNUALLY, ETC) USING A SINGLE SAMPLE AND INDICATOR 
# FUNCTIONS AS DESCRIBED IN STEHMAN ET AL. 2014.

### This script has seven main sections:
### 0) Set global variables
### 1) Read reference strata shapefile and calculate labels per year
### 2) Read yearly strata raster and extract values to the SpatialPoints object (i.e. shapefile) 
### 3) Calculate reference class area proportions and variance of ref samples per strata
### 4) Calculate unbiased standard error for proportion of reference class areas 
### 5) Plot areas and margin of error
### 6) Create some useful tables
### 7) Create some useful plots

require(rgdal)
require(raster)
require(reshape2)
require(ggplot2)
require(gtable)
require(grid)
require(gridExtra)
require(xtable)
require(matrixcalc)
require(extrafont)

loadfonts()

##############################################################################################################
#0) SET VARIABLES/FOLDERS

# Working directory and result (aux) files from the cluster. Move them to Onedrive and exclude from sync!
wd = "/media/paulo/785044BD504483BA/OneDrive/Lab/area_calculation/original_sampling/final_sample"
auxpath = "/media/paulo/785044BD504483BA/test/"
strata_config = "/home/paulo/workflow/multi_scene/7_poststratification/strata_calc_config/"
funcs_path = "/home/paulo/workflow/multi_scene/R_functions/"

setwd(wd)
source(paste0(funcs_path, "area_estimation_fncs.R"))
source(paste0(funcs_path, "plotting_fncs.R"))
source(paste0(strata_config, "input_variables_original_buffer3B.R")) # CHANGE THIS FILE TO RUN WITH OTHER INPUT PARAMETERS!

# Set up important global variables
start = 2001
end = 2016

years = seq(start, end, step) 
periods_long = paste0(years[-length(years)], "-", years[-1])
ref_names = paste("ref_", years, sep="")

# Grid table theme, only used to display some ancillary tables
tt=ttheme_default(core=list(fg_params=list(font="Times", fontface="plain", fontsize=14)),
                  colhead=list(fg_params=list(font="Times", fontface="bold", fontsize=14, parse=TRUE)),
                  rowhead=list(fg_params=list(font="Times", fontface="plain", fontsize=14)))

#############################################################################################################
#1) READ SHAPEFILE AND CALCULATE REFERENCE LABELS PER YEAR
# This section takes the sample shapefile and assigns the proper strata for each year, depending
# on when the model break happened. In this version we use all of the points because when done 
# annually, it does not matter how many changes we found in the TS.  

# Read shapefile with reference strata and change info. 
samples <- readOGR(".", "final_extended_sample_merge_UTM18N_point")

# We need to convert the dates to characters bc they are factors right now
samples$CHGDATE <- as.character(samples$CHGDATE)
# IF WE WANT TO REMOVE OMISSION ERRORS TO ANALYZE IMPACT ON RESULTS
#samples = samples[-c(273, 899, 636),]

# Convert reference sample info into vectors containing proper labels per each of
# the years in our time period. This takes into account that we could be doing the
# analysis every two years or more. 

# Initialize empty vectors to store the data, current change and class code (start with the first!) 
# and name of the fields that store the class codes. Load LUT.
# df stores the labels per year with strata label when there is a change
# df2 stores only the labels per year, with no strata info.
# This whole massive loop could be rewritten using the break_calc function used
# for the break analysis.

rows = nrow(samples)
df = vector()
df2 = vector()
field = vector()
field2 = vector()
codelist = c("CODE1", "CODE2", "CODE3", "CODE4")
lut = read.table(lutpath, header = T, sep = ",")

# Iterate over rows (samples, easier this way)
for (row in 1:rows){
  current_change = 1
  current_code=1
  # Iterate over years
  for (i in 1:length(years)){
  
      # If there are no changes, just use the only class code for both years
      if (is.na(samples$CHGDATE[row]) == TRUE) {
        field[i] = calc_strata(samples$CODE1[row], samples$CODE1[row], lut)
        field2[i] = samples$CODE1[row]
      }
    
      # If there is a change, compare each year to the current change year and update that one accordingly
      else {
        # Get the current date of change (initialized as the first change available)
        chg_date = unlist(strsplit(as.vector(samples$CHGDATE[row]), ','))[current_change] #742
        # Extract the YEAR from the date of change
        chg_year = na.omit(as.numeric(unlist(strsplit(chg_date, "[^0-9]"))))[1]
        
        # If we haven't reached change year yet
        if (years[i] < chg_year) {
          field[i] = calc_strata(samples@data[codelist[current_code]][row,], samples@data[codelist[current_code]][row,], lut) 
          field2[i] = samples@data[codelist[current_code]][row,]
        } 
        
        # If we JUST reached a change year
        else if (years[i] == chg_year) {
          field[i] = calc_strata(samples@data[codelist[current_code]][row,], samples@data[codelist[current_code+1]][row,], lut) 
          field2[i] = samples@data[codelist[current_code+1]][row,]
          # Check if we haven't reached the max number of recorded changes
          if (current_change < samples$NUMCHANGES[row]) {
            current_change = current_change + 1 
          }
          current_code = current_code + 1
        }
        
        # If we went past the last change date, use the last code 
        else if (years[i] > chg_year) {
          field[i] = calc_strata(samples@data[codelist[samples$endcodecol[row]]][row,], samples@data[codelist[samples$endcodecol[row]]][row,], lut)
          field2[i] = samples@data[codelist[current_code]][row,]
        }
      } 
  }
  df <- as.data.frame(rbind(df, field))
  df2 <- as.data.frame(rbind(df2, field2))
  
}

names(df) = ref_names
names(df2) = ref_names

# Recalculate reference labels for the original stratification period, 
# needed when we use a LUT different than the original
strata = vector()
for (row in 1:rows){
  strata[row] = calc_strata(samples@data["CODE1"][row,], samples@data[codelist[samples$endcodecol[row]]][row,], lut)
}

# Attach table to shapefile 
samples@data[,ref_names] <- df
#writeOGR(samples, "sample_yearly_strata", "sample_yearly_strata", driver="ESRI Shapefile", overwrite_layer = T)

#############################################################################################################
# 2) READ ORIGINAL AND ANNUAL STRATA RASTERS AND EXTRACT THEIR VALUES TO THE SHAPEFILE
# Also calculate original strata size, weights, area proportions, and accuracies

# Iterate over names and extract to shapefile. Not required for area estimation,
# only for confusion matrices and accuracies per year

map_names = character()
short_years = substr(years, 3,4) # Get years in two digit format
for (y in 1:(length(years)-1)){
  map_names[y] = paste0(rast_prefix, short_years[y], "_", short_years[y+1], rast_suffix)
  map = raster(paste0(auxpath, map_names[[y]], ".tif"))
  samples = extract(map, samples, sp = TRUE) 
}

# Read original stratification. Done last so that the reference and map fields are contiguous
samples = extract(raster(paste0(auxpath, orig_stratif, ".tif")), samples, sp=TRUE)

# Get unique ref codes and map codes for all the years. Also get unique codes from
# stratification layer (because it may have a buffer that the individual maps don't have)
# Then create a single set of unique codes to be used in the confusion matrices
ref_codes = sort(unique(unlist(samples@data[ref_names])))
map_codes = sort(unique(unlist(samples@data[map_names])))
strat_codes = sort(unique(unlist(samples@data[orig_stratif])))
class_codes = sort(union(ref_codes, map_codes))
class_codes = sort(union(class_codes, strat_codes))

# Create a single variables with the column names for reference labels, map labels and
# original stratification to simplify column indexing and avoid using col numbers
sample_columns = c(ref_names, map_names, orig_stratif)

# Add many correct forest samples to test the effects, if enabled in input file

if(add_samples == TRUE){
  add_samples_suffix = paste0("_nfor", nfor, "_nbuf", nbuf)
  if(nfor > 0){ # If we want to add samples to stable forest class
    df_add <- data.frame(matrix(1,ncol = length(sample_columns), nrow = nfor))
    colnames(df_add) = sample_columns
    samples@data = rbind(samples@data[,sample_columns], df_add)
    strata = c(strata, rep(1, nfor))
  }
  if(nbuf > 0){  # If we want to add samples to the buffer, if it exists
    df_add2 <- data.frame(matrix(1,ncol = length(sample_columns)-1, nrow = nbuf)) # Add forest to ref and map labels
    df_add2 = cbind(df_add2, rep(16, nbuf)) # Add buffer class to stratification column, the code is 16
    colnames(df_add2) = sample_columns
    samples@data = rbind(samples@data[,sample_columns], df_add2)
    strata = c(strata, rep(1, nbuf))
    
    # mat = matrix(1,  ncol=16, nrow=16) ############################# REMOVE/REFORMAT
    # mat[upper.tri(mat)] = 8
    # mat = matrix(rep(t(mat), 1) , ncol=ncol(mat) , byrow=TRUE)
    # df5 = cbind(mat, mat[,2:16], rep(8,16))
    # colnames(df5) = sample_columns
    # samples@data = rbind(samples@data[,sample_columns], df5)
    # strata = c(strata, rep(1, 160))
  }
} else {
    add_samples_suffix = ""
}


# Crosstab final strata and reference strata (for the same period 01-16) 
# Returns a square matrix with all the reference and map codes in the samples

ct = calc_ct(samples[[orig_stratif]], strata, class_codes)

# Load mapped areas for each individual stratification map (total strata sample size) produced from count_pixels.py, 
# REQUIRED for comparison between mapped and estimated areas only. 
# TODO: CONVERT THIS SECTION TO A FUNCTION
mapped_areas_list = list()
# Classes to be removed from the pixcount list bc they are not estimated. 
# shouldn't this include 15 as well? It is not changing the calculations though.
cr_extra = c(13,15) 

for(i in 1:(length(short_years)-1)){
  fname = paste0("strata_", short_years[i], "_", short_years[i+1], pixcount_suffix)
  mapped_areas_list[[i]] = read.csv(paste0(auxpath,fname), header=TRUE, col.names=c("stratum", "pixels"))
  # Get rid of rows we don't need, eg class 15, also done below for ss
  mapped_areas_list[[i]] = mapped_areas_list[[i]][!(mapped_areas_list[[i]]$stratum %in% cr_extra),]
}

# Create empty matrix to store mapped values and fill
mapped_areas = matrix(0, nrow=length(years[2:length(years)]), ncol=nrow(mapped_areas_list[[1]]), byrow=T, dimnames = list(years[2:length(years)], mapped_areas_list[[1]][,1]))

for (i in 1:length(mapped_areas_list)){
  mapped_areas[i,] = mapped_areas_list[[i]][,2]  
}

# Calculate strata weights for each of them
mapped_weights = mapped_areas / rowSums(mapped_areas)

# Convert areas to ha
mapped_areas = mapped_areas * 30^2 / 100^2

# Save mapped areas and strata weights to file.
suffix = paste0("_step", step, "_", lut_name, ".csv")
#write.csv(mapped_areas, file=paste0(savepath, "mapped_areas", suffix))
#write.csv(mapped_weights, file=paste0(savepath, "mapped_areas", suffix))

# Load mapped area of the ORIGINAL Stratification (e.g. 01-16)
ss=read.csv(paste0(auxpath, pixcount_strata), header=TRUE, col.names=c("stratum", "pixels"))

# Filter classes NOT in the list of classes from the pixel count files to be ignored. 
ss = ss[!(ss$stratum %in% cr),] 

# Calculate total number of samples per ORIGINAL stratum. 
# ASSUMES that there is at least one sample per original stratum
strata_pixels = aggregate(samples[[orig_stratif]], by=list(samples[[orig_stratif]]), length)

# Calculate original strata weights and area proportions 
tot_area_pix = sum(ss$pixels) # Assummed to be the same for all maps
tot_area_ha = tot_area_pix * 30^2 / 100^2
str_weight = ss$pixels / tot_area_pix

# Calculate areas and margin of errors for ORIGINAL STRATIFICATION (2001-2016) and save
# This is reusing the functions from the annual stratification bc they provide the same results

prop_out_orig = calc_props_and_vars(samples[[orig_stratif]], strata, samples[[orig_stratif]], ss, strata_pixels, ref_codes) 
se_prop_orig = calc_se_prop(ss, strata_pixels, prop_out_orig[[2]], ref_codes, tot_area_pix)
areas_out_orig = calc_unbiased_area(tot_area_pix, prop_out_orig[[11]], se_prop_orig)
areas_orig = data.frame(t(sapply(areas_out_orig,c)))
colnames(areas_orig) = ref_codes
rownames(areas_orig) = c("area_ha", "area_ci", "area_upper", "area_lower", "margin_error")

# Calculate optimal allocation that could have been used had we had the confusion matrix in advance.
# This would have helped reduce uncertainty in accuracies and areas of the forest to pasture class.
# Given just for reference
cm_prop_square = as.data.frame.matrix(ct * as.vector(str_weight) / strata_pixels$x)
calc_optimal_sample_alloc(cm_prop_square, 8, 1050)

# Calculate accuracies of original stratification, in percentage
acc_out_orig = calc_accuracies(ss, strata_pixels, ref_codes, tot_area_pix,
                                 prop_out_orig[[1]], prop_out_orig[[2]], prop_out_orig[[3]], prop_out_orig[[4]],
                                 prop_out_orig[[5]], prop_out_orig[[6]], 
                                 prop_out_orig[[7]], prop_out_orig[[8]],
                                 prop_out_orig[[9]], prop_out_orig[[10]])

# Format and save tables
nsamp = rowSums(ct)
ct_save = cbind(ct, nsamp, str_weight)
ss_filtered = ss[ss$stratum %in% ref_codes,]
ss_filtered$area_ha = ss_filtered$pixels * 30^2 / 100^2
areas_orig["map_bias",] = ss_filtered$area_ha - areas_orig["area_ha",]

suffix = paste0("_step", step, "_", lut_name, add_samples_suffix, ".csv")
#write.csv(ct_save, file=paste0(savepath, "confusion_matrix_counts_and_weights", suffix))
#write.csv(prop_out_orig[[11]], file=paste0(savepath, "confusion_matrix_area_prop", suffix))
#write.csv(cbind(acc_out_orig[[4]], acc_out_orig[[7]], acc_out[[1]]), 
#          col.names = c("users", "producers", "overall"),
#          file=paste0(savepath, "accuracies", suffix))
#write.csv(areas_orig,  file=paste0(savepath, "area_ha_orig_strata", add_samples_suffix, ".csv"))


##############################################################################################################
# 3) CALCULATE REFERENCE CLASS AREA PROPORTIONS AND VARIANCE OF REFERENCE SAMPLES PER STRATA 
# 4) CALCULATE UNBIASED STANDARD ERROR FOR PROPORTION OF REFERENCE CLASS AREAS

# Initialize variables for area proportions, variances, etc (Step 1)
ref_prop_list = list()
ref_var_list = list()
map_prop_list = list()
map_var_list = list()
mapref_prop_list = list()
mapref_var_list = list()
overall_acc_prop_list = list()
overall_acc_var_list = list()
users_cov_list = list()
producers_cov_list = list()
filtered_ss = list()
overall_accs = vector()
overall_accs_min = vector()
overall_accs_max = vector()

matrix_names = list(periods_long, strata_names)
area_prop = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)

# Initialize empty matrix to store standard error proportions (Step 2)
se_prop = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)

# Initialize matrices for area calculations (Step 3)
area_ha = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
area_ci = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
area_upper = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
area_lower = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
margin_error = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)

# Initialize matrices for accuracies
usr_acc = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
usr_acc_lower = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
usr_acc_upper = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
prod_acc = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
prod_acc_lower = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)
prod_acc_upper = matrix(0, nrow=length(years)-1, ncol=length(ref_codes), dimnames=matrix_names)

#' Run all the calculations. Call the function for every reference year we want and get area proportions
#' and sample variance, then standard errors on proportions, then areas and their confidence intervals and
#' margin of errors. NOTE the double square brackets to allow for substitution.  

for (y in (1:(length(years)-1))){
  # Compare year strata with year reference, e.g strata_01_02 vs reference_2001.
  
  prop_out = calc_props_and_vars(samples[[orig_stratif]], samples[[ref_names[y]]], samples[[map_names[y]]], 
                            ss, strata_pixels, ref_codes) 
  
  # Assign outputs of Step 1 - Proportions and variances for areas and accuracies
  ref_prop_list[[y]] = prop_out[[1]]
  ref_var_list[[y]] = prop_out[[2]]
  map_prop_list[[y]] = prop_out[[3]]
  map_var_list[[y]] = prop_out[[4]]
  mapref_prop_list[[y]] = prop_out[[5]]
  mapref_var_list[[y]] = prop_out[[6]]
  overall_acc_prop_list[[y]] = prop_out[[7]]
  overall_acc_var_list[[y]] = prop_out[[8]]
  users_cov_list[[y]] = prop_out[[9]]
  producers_cov_list[[y]] = prop_out[[10]]
  area_prop[y,] = prop_out[[11]]

  # Run step two - Calculate standard error of area proportions
  se_prop[y,] = calc_se_prop(ss, strata_pixels, ref_var_list[[y]], 
                             ref_codes, tot_area_pix)
  se_area_ha = se_prop * tot_area_ha
  
  # Run and assign outputs of Step 3 - Area estimation and margin of error
  areas_out = calc_unbiased_area(tot_area_pix, area_prop[y,], se_prop[y,]) 
  area_ha[y,] = areas_out[[1]]
  area_ci[y,] = areas_out[[2]]
  area_upper[y,] = areas_out[[3]]
  area_lower[y,] = areas_out[[4]]
  margin_error[y,] = areas_out[[5]]
  
  # Step 4 - Calculate accuracies and their CI's (SE's not returned) in percentage
  accuracies_out = calc_accuracies(ss, strata_pixels, ref_codes, tot_area_pix,
                                   ref_prop_list[[y]], ref_var_list[[y]], map_prop_list[[y]], map_var_list[[y]],
                                   mapref_prop_list[[y]], mapref_var_list[[y]], 
                                   overall_acc_prop_list[[y]], overall_acc_var_list[[y]],
                                   users_cov_list[[y]], producers_cov_list[[y]])
  overall_accs[y] = accuracies_out[[1]]
  overall_accs_min[y] = accuracies_out[[2]]
  overall_accs_max[y] = accuracies_out[[3]]
  usr_acc[y,] = accuracies_out[[4]]
  usr_acc_lower[y,] = accuracies_out[[5]]
  usr_acc_upper[y,] = accuracies_out[[6]]
  prod_acc[y,] = accuracies_out[[7]]
  prod_acc_lower[y,] = accuracies_out[[8]]
  prod_acc_upper[y,] = accuracies_out[[9]]
  
}

# Create confusion matrices per year between reference and map labels.
# Useful for interpreting results

cm_list = list()
for (y in 1:(length(years) - 1)){
  cm_list[[y]] = calc_ct(samples[[map_names[y]]], samples[[ref_names[y]]], class_codes)
}

# Calculate map biases per period, filter only classes of interest

map_bias = mapped_areas[, as.character(ref_codes)] - area_ha
colnames(map_bias) = strata_names

# Write results to csv (areas and accuracies)

suffix = paste0("_step", step, "_", lut_name, add_samples_suffix, ".csv")
# write.csv(area_ha, file=paste0(savepath, "area_ha", suffix))
# write.csv(area_ci, file=paste0(savepath, "area_ci", suffix))
# write.csv(area_lower, file=paste0(savepath, "area_lower", suffix))
# write.csv(area_upper, file=paste0(savepath, "area_upper", suffix))
# write.csv(map_bias, file=paste0(savepath, "map_bias", suffix))
# write.csv(se_area_ha, file=paste0(savepath, "se_area_ha", suffix))
# write.csv(margin_error, file=paste0(savepath, "margin_error", suffix))
# write.csv(cbind(overall_accs, overall_accs_min, overall_accs_max), file=paste0(savepath, "overall_accuracies_minmax", suffix))
# write.csv(usr_acc , file=paste0(savepath, "users_accuracies", suffix))
# write.csv(usr_acc_lower, file=paste0(savepath, "users_accuracies_min", suffix))
# write.csv(usr_acc_upper, file=paste0(savepath, "users_accuracies_max", suffix))
# write.csv(prod_acc , file=paste0(savepath, "producers_accuracies", suffix))
# write.csv(prod_acc_lower, file=paste0(savepath, "producers_accuracies_min", suffix))
# write.csv(prod_acc_upper, file=paste0(savepath, "producers_accuracies_max", suffix))

# Calculate areas in kha for plots and tables
area_kha = area_ha / 1000
area_upper_kha = area_upper / 1000
area_lower_kha = area_lower / 1000
se_area_kha = se_area_ha / 1000


##############################################################################################################
# 5) PLOT AREAS AND MARGINS OF ERROR

# Vector of max and min y axis values for pontus modes
# Selected to guarantee that one of the breaks (6 total) is zero
maxy_vect1 = c(12000, 45000000, 2000000, 180000, 3600000, 2000000, 1000000, 
               180000, 60000, 60000, 60000)
maxy_vect2 = c(12000, 45000000, 4500000, 400000, 4500000, 4500000, 4500000, 
               400000, 400000, 400000, 400000)
miny_vect1 = c(0, 37000000, 0,0,0,0,0,0,0,0,0)
miny_vect2 = c(-3000, 0, 0, -100000, 0, 0, 0, -100000, -100000, -100000, -100000)

# Limits in kha
maxy_vect1 = maxy_vect1 / 1000
maxy_vect2 = maxy_vect2 / 1000
miny_vect1 = miny_vect1 / 1000
miny_vect2 = miny_vect2 / 1000

# Create each plot in the original order
plot_list1 = list()
plot_list2 = list()
plot_list3 = list()
gpl1 = list()
gpl2 = list()
gpl3 = list()
mep1 = list()
mep2 = list()
mep3 = list()
widths1 = list()
widths2 = list()
widths3 = list()
widths1me = list()
widths2me = list()
widths3me = list()
plot_periods = years[-1]
plot_periods = seq(2,14,2)
plot_labels = mapply(paste0, letters[seq(1,11)], ") ", strata_names)

# Other to other was removed from paper figures
label_letters2 = c("a", "a", "c", "b", "d", "e", "f", "g", "h", "i", "j")
plot_labels2 = mapply(paste0, label_letters2, ") ", strata_names)


tot_area_kha = tot_area_ha / 1000
mapped_areas_kha = mapped_areas / 1000

# Get AREA PLOTS  in the original order, for both plot modes plus regular
for(i in 1:length(strata_names)){
  plot_list1[[i]] = plot_areas(tot_area_kha, plot_periods, area_kha[,i], 
                               area_lower_kha[,i], area_upper_kha[,i], mapped_areas_kha[,i],
                               margin_error[,i], miny_vect1[i], maxy_vect1[i], plot_labels2[i], plotmode=1)  
  plot_list2[[i]] = plot_areas(tot_area_kha, plot_periods, area_kha[,i], 
                               area_lower_kha[,i], area_upper_kha[,i], mapped_areas_kha[,i],
                               margin_error[,i], miny_vect2[i], maxy_vect2[i], strata_names[i], plotmode=2)  
  plot_list3[[i]] = plot_areas(tot_area_kha, plot_periods, area_kha[,i], 
                               area_lower_kha[,i], area_upper_kha[,i], mapped_areas_kha[,i],
                               margin_error[,i], miny_vect2[i], maxy_vect2[i], strata_names[i], plotmode=3)  

  
  gpl1[[i]] = ggplotGrob(plot_list1[[i]][[1]])
  gpl2[[i]] = ggplotGrob(plot_list2[[i]][[1]])
  gpl3[[i]] = ggplotGrob(plot_list3[[i]][[1]])
  mep1[[i]] = ggplotGrob(plot_list1[[i]][[2]])
  mep2[[i]] = ggplotGrob(plot_list2[[i]][[2]])
  mep3[[i]] = ggplotGrob(plot_list3[[i]][[2]])
  widths1[[i]] = gpl1[[i]]$widths[2:5]
  widths2[[i]] = gpl2[[i]]$widths[2:5]
  widths3[[i]] = gpl3[[i]]$widths[2:5]
  widths1me[[i]] = mep1[[i]]$widths[2:5]
  widths2me[[i]] = mep2[[i]]$widths[2:5]
  widths3me[[i]] = mep3[[i]]$widths[2:5]
}

# Calculate max width among all the grobs for each case and use that value for all of them
# This ensures the plotted areas match despite different y axis widths.
maxwidth1 = do.call(grid::unit.pmax, widths1)
maxwidth2 = do.call(grid::unit.pmax, widths2)
maxwidth3 = do.call(grid::unit.pmax, widths3)
maxwidth1me = do.call(grid::unit.pmax, widths1me)
maxwidth2me = do.call(grid::unit.pmax, widths2me)
maxwidth3me = do.call(grid::unit.pmax, widths3me)

for (i in 1:length(gpl1)){
  gpl1[[i]]$widths[2:5] = as.list(maxwidth1)
  gpl2[[i]]$widths[2:5] = as.list(maxwidth2)
  gpl3[[i]]$widths[2:5] = as.list(maxwidth3)
  mep1[[i]]$widths[2:5] = as.list(maxwidth1me)
  mep2[[i]]$widths[2:5] = as.list(maxwidth2me)
  mep3[[i]]$widths[2:5] = as.list(maxwidth3me)
}

gpar_settings = gpar(fontsize=7, 
                     fontfamily="Times New Roman", 
                     fontface="bold")
left_axlabel = textGrob("Area [kha]", gp=gpar_settings, rot=90)
right_axlabel = textGrob("Percentage of total area", gp=gpar_settings, rot=-90)
bottom_axlabel = textGrob("Year", gp=gpar_settings)

# Arrange AREA PLOTS in the NEW grouping order and save multiplots
pontus_multiplot1 = grid.arrange(gpl1[[2]], gpl1[[4]], 
                                 gpl1[[3]], gpl1[[5]], gpl1[[6]], gpl1[[7]],
                                 gpl1[[8]], gpl1[[9]], gpl1[[10]], gpl1[[11]],ncol=2, 
                                 left=left_axlabel, right=right_axlabel, bottom=bottom_axlabel)

# Formatted for article
outfile=paste0(savepath, "ALL_Pontus1_step", step, "_kha_", lut_name, ".pdf")
ggsave(outfile, plot=pontus_multiplot1,  width = 140, height = 160, units='mm') 
embed_fonts(outfile)

pontus_multiplot2 = grid.arrange(textGrob(""), gpl2[[1]], gpl2[[2]], gpl2[[4]], 
                                 gpl2[[3]], gpl2[[5]], gpl2[[6]], gpl2[[7]],
                                 gpl2[[8]], gpl2[[9]], gpl2[[10]], gpl2[[11]],ncol=4, 
                                 left=left_axlabel, right=right_axlabel, bottom=bottom_axlabel)

ggsave(paste0(savepath, "ALL_Pontus2_step", step, "_kha_", lut_name, ".png"), 
       plot=pontus_multiplot2,  width = 20, height = 10) 

# Arrange MARGIN OF ERROR PLOTS in the NEW grouping order and save multiplots
left_axlabel_me = textGrob("Margin of error [%]", gp=gpar(fontsize=12, fontface="bold"), rot=90)
pontus_multiplotme1 = grid.arrange(textGrob(""), mep1[[1]], mep1[[2]], mep1[[4]], 
                                   mep1[[3]], mep1[[5]], mep1[[6]], mep1[[7]],
                                   mep1[[8]], mep1[[9]], mep1[[10]], mep1[[11]],ncol=4,
                                   left=left_axlabel_me,  bottom=bottom_axlabel)

ggsave(paste0(savepath, "ALL_Pontus1me_step", step, "_kha_", lut_name, ".png"), 
       plot=pontus_multiplotme1,  width = 20, height = 10) 


# Save forest to pasture plot separately for the paper
y1_label = textGrob("Area and 95% CI [kha]", gp=gpar_settings, rot=90)
y2_label = textGrob("Percentage of total area", gp=gpar_settings, rot=-90)
x_label = textGrob("Time", gp=gpar_settings)
 
# Override xlabels to show full years
f2p = plot_list1[[8]][[1]] + ylab("") + xlab("") + ggtitle("") + 
  scale_x_continuous(breaks=seq(1,length(years[-1] -1)), labels=years[-1] -1, minor_breaks = NULL)
f2p_plot = grid.arrange(ggplotGrob(f2p), left=y1_label, right=y2_label, bottom=x_label)
outfile=paste0(savepath, "f2p_for_paper", step, "_", lut_name, ".pdf")
ggsave(outfile, plot=f2p_plot,  width = 90, height = 90, units="mm") 
embed_fonts(outfile)

# Save individual plots with margin of error
ap = list()
mep = list()

for(i in 1:length(strata_names)){
  ap[[i]] = ggplotGrob(plot_list3[[i]][[1]])
  mep[[i]] = ggplotGrob(plot_list3[[i]][[2]])
  g = rbind(ap[[i]], mep[[i]], size="first") 
  g$widths = unit.pmax(ap[[i]]$widths, mep[[i]]$widths)
  
  filename = paste0(savepath, strata_names[[i]], "_areas_me_", lut_name, "_newplots.png")
  ggsave(filename, plot=g,  width = 12, height = 15) 
}


##############################################################################################################
### CREATE TABLES FOR PAPER

# TABLES OF AREAS, STANDARD ERRORS AND MARGINS OF ERROR
print(xtable(t(area_kha), digits=1,type = "latex",sanitize.text.function=function(x){x}))
print(xtable(t(se_area_kha), digits=1,type = "latex",sanitize.text.function=function(x){x}))
print(xtable(t(margin_error*100), digits=1,type = "latex",sanitize.text.function=function(x){x}))

# 6) CREATE SOME USEFUL TABLES - NEEDS REWRITTING

# Export table of  sample count, areas, percentages

stratum_percentages=round(ss$pixels / tot_area_pix * 100, digits=3) 
strata_table = as.data.frame(cbind(stratum_areas, stratum_percentages, strata_pixels$x))
strata_table$stratum_areas = format(strata_table$stratum_areas, scientific = FALSE, big.mark = ",")
rownames(strata_table) = orig_strata_names 
# Need to escape special characters, including backslash itself (e.g. $\\alpha$)
colnames(strata_table) = c("Area [ha]", "Area / $W_h$ [\\%]", "Sample size ($n_h$)") 
# Create table in Latex instead, and produce the pdf there, much easier than grid.table
print(xtable(strata_table, digits=c(0,2,2,0)),type = "latex",sanitize.text.function=function(x){x})


# Calculate reference sample count per year. Initialize zero matrix with year * class dims and proper row and column names
ref_sample_count = matrix(0, nrow=length(years), ncol=length(ref_codes), dimnames=list(years, ref_codes))

for (f in 1:(length(ref_names))){
  # Get table, then check if unique classes is in that table, then use the boolean to assign values
  a = table(samples[[ref_names[f]]])
  class_check = ref_codes %in% names(a)
  ref_sample_count[f,class_check] = a
}

# Format and show/save
grid.newpage()
tt =  ttheme_default(base_size=20)
grid.table(round(ref_sample_count), theme=tt) 
png(paste0(savepath, "numchange_strata_ref.png"), width=1000, height = 1000, units = "px"); grid.table(ref_sample_count, theme=tt); dev.off()

## BREAK ANALYSIS
## Calculate when break occurred in maps and compare to break in reference samples

# Function to return the indices of the locations where a there is a change in 
# class code of a vector. Useful for detecting years of change of reference and
# map labels. 

break_calc = function(values){
  out = vector()
  unq = unique(as.numeric(values))
  # When there are no changes
  if (length(unq) == 1) {
    out = 0
  # When there is one or more changes
  } else {
    out = c(1 + which(diff(as.numeric(values))!=0))
  }
  return(out)
}
    
        
# We need to load the original maps in order to avoid having to reclassify
# the strata rasters. 

alt_samples = samples
rast_names = character()
for (y in 1:length(years)){
 rast_names[y] = paste0(years[y],"_final_crop")
 map = raster(paste0(auxpath, rast_names[y], ".tif"))
 alt_samples = extract(map, alt_samples, sp = TRUE) 
}

# After reading, prepend an X to be able to call columns by name
rast_names = paste0("X", rast_names)

# Get break indices (column) for REFERENCE data in ORIGINAL class codes (e.g no strata codes)
bi_ref = apply(df2, MARGIN = 1, FUN = break_calc)
# Get the max number of elements in a subelement of the list
max_l <- max(sapply(bi_ref, length))
# Fill with NA's using the max number of elements
l <- lapply(bi_ref, function(v) { c(v, rep(NA, max_l - length(v)))})
# Bind into a dataframe
bi_ref = do.call(rbind, l)

# Get break indices (column) for the original MAPPED data. 

bi_map = apply(alt_samples@data[rast_names], MARGIN = 1, FUN = break_calc) 
# Get the max number of elements in a subelement of the list
max_l <- max(sapply(bi_map, length))
# Fill with NA's using the max number of elements
l <- lapply(bi_map, function(v) { c(v, rep(NA, max_l - length(v)))})
# Bind into a dataframe
bi_map = do.call(rbind, l)
# Substract 1 and replace -1 with zero. We need this beause the maps really represent the 
# dynamic of the year before them. This also allows us to index from STRATA maps directly
# (because the max == 15)
bi_map = bi_map - 1
bi_map[bi_map == -1] = 0

# Get rows where there is only one or no changes in the REFERENCE AND MAP labels
# to simplify the analysis.
# Use the NUMCHANGES column because it is a better source of break info

ref_chg = samples@data["NUMCHANGES"] <= 1 
numchg_map = apply(bi_map, MARGIN = 1, FUN = function(x) length(unique(na.omit(x))))
map_chg = numchg_map <= 1
subind = which(ref_chg & map_chg)
subind2 = which(!(ref_chg & map_chg)) # Get the complementary indices, we will need them

# Subset reference and map labels based on those indices
# Remove all the NA's (effectively making it a vector)
bi_ref_sub = bi_ref[subind, ]
bi_ref_sub = bi_ref_sub[!is.na(bi_ref_sub)]

bi_map_sub = bi_map[subind, ]
bi_map_sub = bi_map_sub[!is.na(bi_map_sub)]

# Function to get the strata codes using those indices, both for reference and maps.
# There must be a way to vectorize that but I don't see it
get_break_strata = function(rw, index){
  # When there is no change for reference (=0)
  if(index <= 0){
    code = rw[,1]
  } else {
    code = rw[,index]  
  }
  return(code)
}

# Initialize vectors and get reference and map labels using the column indices
break_ref = vector(length = length(subind))
break_map = vector(length = length(subind))

for(i in 1:length(subind)){
  break_ref[i] = get_break_strata(df[subind[i],], bi_ref_sub[i])  
  break_map[i] = get_break_strata(samples@data[subind[i], map_names], bi_map_sub[i])  
}

# Create dataframe with ref breaks map breaks and their corresponding labels.
# Create a larger df with these results and the row id so that we can go back to the 
# original samples easily. 
break_ref_df = cbind(bi_ref_sub, break_ref)
break_map_df = cbind(bi_map_sub, break_map)
break_compare = as.data.frame(cbind(subind, break_ref_df, break_map_df, samples@data[subind, "PTRW"]))
colnames(break_compare) = c("row_id", "ref_year", "ref_label", "map_year", "map_label", "ptrw")

# Convert the indices to actual years to make subsequent analysis easier
# (Just add 2000 and replace 2000 with zeros!)
break_compare$ref_year = break_compare$ref_year + 2000
break_compare$ref_year[break_compare$ref_year == 2000] = 0

break_compare$map_year = break_compare$map_year + 2000
break_compare$map_year[break_compare$map_year == 2000] = 0

# Create confusion matrix of breaks and labels between map and reference
cm_breaks = calc_ct(break_compare$map_year, break_compare$ref_year)
cm_labels = calc_ct(break_compare$map_label, break_compare$ref_label)

# Add row and column totals and display/save
cm_breaks = cbind(cm_breaks, total=rowSums(cm_breaks))
cm_breaks = rbind(cm_breaks, total=colSums(cm_breaks)) 

# Format table and display
tt= ttheme_default(base_size=18)
grid.newpage()
grid.table(round(cm_breaks, digits=2), theme=tt)
#png("numchange_strata_ref.png", width=1000, height = 1000, units = "px"); grid.table(cm_breaks, theme=tt); dev.off()

# Find records for any given set of map year (from 1 to 16) and change year and get comparison of labels for it
# Eg show label distribution for BREAK OMISSION ERRORS. 
records = break_compare[break_compare$map_year == 0 & break_compare$ref_year != 0,]
calc_ct(records$map_label, records$ref_label)

# Eg show label distribution for BREAK COMMISSION ERRORS
records = break_compare[break_compare$map_year != 0 & break_compare$ref_year == 0,]
calc_ct(records$map_label, records$ref_label)

# Eg show label distribution for EXACT CHANGE MATCH BREAK DETECTION
records = break_compare[(break_compare$map_year == break_compare$ref_year) & (break_compare$map_year !=0 & break_compare$ref_year != 0),]
calc_ct(records$map_label, records$ref_label)

# Eg show label distribution for STABLE MATCH
records = break_compare[break_compare$map_year == 0 & break_compare$ref_year == 0,]
calc_ct(records$map_label, records$ref_label)

# Can also be done for labels, to find years of break
records = break_compare[break_compare$map_label == 8 & break_compare$ref_label == 8,]
calc_ct(records$map_year, records$ref_year)

# Analyze strata breaks vs ref breaks per path/row
break_compare$breakdif = break_compare$ref_year - break_compare$map_year

# Calculate type of break difference per sample: ommited change, commited change, 
# detected change with small ofset in timing (either positive or negative), 
# exact change match (first change detected in the same year) or stable match
# (no changes detected in reference or maps)

break_compare$type[break_compare$breakdif > 2000] = "omission"
break_compare$type[break_compare$breakdif < -2000] = "comission" 
break_compare$type[(break_compare$breakdif > 0) & (break_compare$breakdif < 2000)] = "pos_offset" 
break_compare$type[(break_compare$breakdif < 0) & (break_compare$breakdif > -2000)] = "neg_offset" 
# Assume rows == 0 are exact change matches, then overwrite those that are stable matches
break_compare$type[break_compare$breakdif == 0] = "exact_change_match" 
break_compare$type[(break_compare$ref_year == 0) & (break_compare$map_year == 0)] = "stable_match"

# Create new dataframe with the original number of rows and populate with the
# results. Fill in the rows with no results (those that we didnt analyze)
long_break_compare = data.frame(matrix(ncol = ncol(break_compare), nrow = nrow(df)))
colnames(long_break_compare) = colnames(break_compare)
long_break_compare[subind, ] = break_compare
long_break_compare[subind2, "type"] = "not_analyzed"
long_break_compare[subind2, "ptrw"] = samples@data[subind2,"PTRW"]

# Aggregate results and calculate total points per path-row. 
breakdif_count = aggregate(long_break_compare$type, by=list(long_break_compare$ptrw), FUN="summary")
total_ptrw_pts = apply(breakdif_count[,2], MARGIN = 1, FUN = "sum")

# # Calculate number of each type of change as ratios of the total
bd_ratios = t(apply(breakdif_count[,2], MARGIN = 1, function(x) x/sum(x))) 
rownames(bd_ratios) = breakdif_count$Group.1
bd_ratios_df = cbind(bd_ratios, total_ptrw_pts)

# Write aggregated results to CSV and row results to shapefile
#write.csv(bd_ratios_df, paste0(savepath, "LC_change_ratios_pathrow.csv"))
#samples@data[,names(long_break_compare)] <- long_break_compare
#writeOGR(samples, paste0(savepath, "sample_yearly_strata_break_analysis"), "sample_yearly_strata_break_analysis", driver="ESRI Shapefile", overwrite_layer = T)

# Compare total breaks detected per years
total_breaks = cbind(cm_breaks["total",], cm_breaks[,"total"])
colnames(total_breaks) = c("ref_breaks", "map_breaks")


##############################################################################################################
# 6) CREATE SOME MORE PLOTS, NEEDS TO BE REWRITTEN


## Plot net regrowth (net secondary forest). Yearly loss in 5 equals yearly gain in 14

net_rg = area_ha[,6] + area_ha[,10] + area_ha[,9] - area_ha[,11]

# Get only gain classes 
regr_area = area_ha[,c(6,9,10)]
names(regr_area) = c("Stable secondary forest", "Forest to secondary forest", "Other gains of secondary forest") #, "Loss of regrowth")

# Melt and plot
regr_area_melt = melt(as.matrix(regr_area))

regr_plot <- ggplot(regr_area_melt, aes(x=Var1,y=value,group=Var2,fill=Var2)) + 
  geom_area(position="stack", alpha=0.8) + #geom_line(aes(x=Var1, y=732031.3), linetype=2) + 
  scale_x_continuous(breaks=years[2:length(years)], minor_breaks = NULL)  + 
  scale_y_continuous(labels=function(n){format(n, scientific = FALSE, big.mark = ",")}) + 
  ylab("Area [ha]") + xlab("Years")+
  scale_fill_brewer(palette="GnBu",  breaks=levels(as.factor((regr_area_melt$Var2))), guide = guide_legend(reverse=T)) + 
  theme(axis.title=element_text(size=15), axis.text=element_text(size=13), legend.text=element_text(size=13),
  legend.title=element_blank()) #+ 
  #annotate("segment", x = 2016, xend=2016, y = 732031.3, yend = 416495.7, colour = "black", linetype=2) +
  #annotate("text", x = 2014,  y = 600000, label="Loss of regrowth")  

print(regr_plot)
#ggsave("net_regrowth_secondaryforest.png", plot=regr_plot, device="png") 


## Plot forest converstion to pasture and secondary forest 
# Forest loss = total area of classs 8 + 9 + whatever is left
for_loss = area_ha[,c(8,9)]
names(for_loss) = c("Forest to pasture", "Forest to secondary forest")

# Melt and plot. 
for_loss_melt = melt(as.matrix(for_loss))
forest_loss_plot <- ggplot(for_loss_melt, aes(x=Var1,y=value,group=Var2,fill=Var2)) + 
  geom_area(position="stack", alpha=0.8) + 
  scale_x_continuous(breaks=years[2:length(years)], minor_breaks = NULL)  + 
  scale_y_continuous(labels=function(n){format(n, scientific = FALSE, big.mark = ",")}) + 
  ylab("Loss of primary forest [ha]") + xlab("Years") +
  scale_fill_brewer(palette="GnBu", breaks=levels(for_loss_melt$Var2), guide = guide_legend(reverse=T)) + 
  theme(legend.title=element_blank()) +
  theme(axis.title=element_text(size=15), axis.text=element_text(size=13), legend.text=element_text(size=13))
  
print(forest_loss_plot)
#ggsave("forest_loss.png", plot=forest_loss_plot, device="png") 


##############################################################################################################
# MISCELANEOUS
# Plot number of forest to pasture reference samples over time.
fpc = vector()
for (f in ref_names){
  a = samples@data[,f] == 8
  fpc = cbind(fpc, sum(a))
}

dtf = as.data.frame(cbind(years, t(fpc)))
colnames(dtf) = c("Years", "Count")
plot(dtf,type="l")

